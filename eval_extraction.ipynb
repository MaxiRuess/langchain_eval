{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langsmith langchain-openai langchain-core langchain-community pydantic python-dotenv openai\n",
    "%pip install --upgrade langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LangSmith version: 0.3.5\n"
     ]
    }
   ],
   "source": [
    "import langsmith\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Current LangSmith version:\", langsmith.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langsmith import wrappers, Client\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = wrappers.wrap_openai(OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed all docs...\n",
      "```json\n",
      "{\n",
      "  \"Products and Services\": [\n",
      "    \"iPhone\",\n",
      "    \"Mac\",\n",
      "    \"iPad\",\n",
      "    \"Wearables, Home and Accessories\",\n",
      "    \"AppleCare\",\n",
      "    \"Cloud Services\",\n",
      "    \"Digital Content\",\n",
      "    \"Payment Services\"\n",
      "  ],\n",
      "  \"Risk Factors\": [\n",
      "    \"Adverse global and regional economic conditions\",\n",
      "    \"Political events, trade and other international disputes\",\n",
      "    \"Natural disasters, public health issues, industrial accidents\",\n",
      "    \"Geopolitical tensions and conflict\",\n",
      "    \"Technological change and competition\",\n",
      "    \"Supply chain disruption and component shortages\",\n",
      "    \"Currency fluctuations and foreign exchange risk\",\n",
      "    \"Potential financial and legal liabilities from regulatory changes\"\n",
      "  ],\n",
      "  \"IRS Employer ID Number\": [\n",
      "    \"94-2404110\"\n",
      "  ],\n",
      "  \"Company Address\": [\n",
      "    \"One Apple Park Way, Cupertino, California 95014\"\n",
      "  ],\n",
      "  \"Earnings Per Share (Basic)\": [\n",
      "    \"6.11\"\n",
      "  ],\n",
      "  \"Net Income\": [\n",
      "    \"93,736\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "class UsefulInformation(BaseModel):\n",
    "    products_and_services: list[str] = Field(description=\"A list of products and services provided by the company\")\n",
    "    risk_factors: list[str] = Field(description=\"A list of risk factors described in the document\")\n",
    "    irs_employer_id_number: list[str] = Field(description=\"The IRS Employer Identification Number of the company\")\n",
    "    company_address: list[str] = Field(description=\"The address of the company\")\n",
    "    earnings_per_share_basic: list[float] = Field(description=\"The basic earnings per share of the company\")\n",
    "    net_income: list[str] = Field(description=\"The net income of the company\")\n",
    "    \n",
    "    \n",
    "def extract_information(docs):\n",
    "    prompt = \"\"\"\n",
    "    The text below is an excerpt from a 10-K report. You must extract specific information and return it in a structured format.\n",
    "    \n",
    "    CRITICAL INSTRUCTIONS:\n",
    "    1. AVOID DUPLICATES: Never include duplicate items in any list\n",
    "    2. BE CONCISE: Keep each item brief and to the point\n",
    "    3. VALIDATE: Each piece of information must be explicitly stated in the text, do not make assumptions\n",
    "    4. FORMAT: All fields must be lists, even if empty or single item\n",
    "    \n",
    "    Examples of GOOD responses:\n",
    "    - Products: [\"Google Search\", \"Google Cloud\", \"Android\"]\n",
    "    - Address: [\"1600 Amphitheatre Parkway, Mountain View, CA 94043\"]\n",
    "    - Phone: [\"+1 650-253-0000\"]\n",
    "    \n",
    "    Examples of BAD responses (avoid these):\n",
    "    - Duplicates: [\"Google Search\", \"Search by Google\", \"Google Search Engine\"]\n",
    "    - Too verbose: [\"Google Search is a web search engine that allows users to search the World Wide Web...\"]\n",
    "    - Made up data: Do not include information unless explicitly found in the text\n",
    "    \n",
    "    Please extract:\n",
    "    1. Products and Services: List unique products/services (max 10 items)\n",
    "    2. Risk Factors: List unique, critical risks (max 10 items)\n",
    "    3. IRS Employer ID Number: List any EIN found\n",
    "    4. Company Address: List primary address of the company\n",
    "    5. Earnings Per Share (Basic): List basic EPS figure\n",
    "    6. Net Income: List net income figure\n",
    "\n",
    "    Text from the 10-K report:\n",
    "    {doc}\n",
    "    \"\"\"\n",
    "    \n",
    "    try: \n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt.format(doc=docs)},\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in structured output LLM call: {str(e)}\")\n",
    "        print(f\"Error type: {type(e)}\")\n",
    "        return UsefulInformation(\n",
    "            products_and_services=[],\n",
    "            risk_factors=[],\n",
    "            irs_employer_id_number=[],\n",
    "            company_address=[],\n",
    "            earnings_per_share_basic=[],\n",
    "            net_income=[]\n",
    "        )\n",
    "\n",
    "\n",
    "def process_all_docs(): \n",
    "    \n",
    "    all_text = load_pdf('/Users/maximilianruess/Documents/GitHub/llm-sdlc/langchain_eval/docs/aapl.pdf')\n",
    "    results = extract_information(all_text)\n",
    "    \n",
    "    print(\"processed all docs...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "aggregated_info = process_all_docs()\n",
    "print(aggregated_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"10-k extraction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable \n",
    "\n",
    "client = Client()\n",
    "\n",
    "@traceable\n",
    "def target(inputs: dict) ->dict: \n",
    "    response = openai_client.chat.completions.parse(\n",
    "        model=\"gpt-4o\", \n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": inputs[\"input\"][0][\"content\"]}, \n",
    "            \n",
    "        ],\n",
    "        response_format=UsefulInformation,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def format_objects_for_llm_judge(obj1, obj2):\n",
    "    \"\"\"Formats two objects into natural language for easier LLM comparison.\"\"\"\n",
    "    def format_single_object(obj, object_name):\n",
    "        if isinstance(obj, str):\n",
    "            obj = json.loads(obj)\n",
    "        formatted_sections = []\n",
    "        formatted_sections.append(f\"\\n{object_name} contains the following information:\")\n",
    "        sorted_keys = sorted(obj.keys())\n",
    "        for key in sorted_keys:\n",
    "            values = obj[key]\n",
    "            readable_key = key.replace('_', ' ').capitalize()\n",
    "            if isinstance(values, list):\n",
    "                if len(values) == 1:\n",
    "                    formatted_sections.append(f\"\\n{readable_key}: {values[0]}\")\n",
    "                else:\n",
    "                    items = '\\n  - '.join(values)\n",
    "                    formatted_sections.append(f\"\\n{readable_key}:\\n  - {items}\")\n",
    "            else:\n",
    "                formatted_sections.append(f\"\\n{readable_key}: {values}\")\n",
    "        \n",
    "        return '\\n'.join(formatted_sections)\n",
    "\n",
    "    object1_text = format_single_object(obj1, \"Actual Output\")\n",
    "    object2_text = format_single_object(obj2, \"Reference Output\")\n",
    "    return [object1_text, object2_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(run_type=\"llm\")\n",
    "def run_llm_judge(formatted_text):\n",
    "    class Score(BaseModel): \n",
    "        \"\"\"Evaluate how well an extracted output matches reference ground truth for 10-k document information\"\"\"\n",
    "        \n",
    "        accuracy: bool = Field(\n",
    "                        description=(\n",
    "                \"A binary score (0 or 1) that indicates whether the model's extraction adequately matches the reference ground truth. \"\n",
    "                \"Score 1 if the model's output captures the same essential business information as the reference extraction, even if \"\n",
    "                \"expressed differently. The goal is to verify that the model successfully extracted similar key business information \"\n",
    "                \"as found in the reference ground truth, not to ensure identical representation.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        reason: str = Field(\n",
    "            description=(\n",
    "                \"An explanation of how well the model's extraction aligns with the reference ground truth. Consider how effectively \"\n",
    "                \"the model captured the same key business information, financial data, and risk factors as the reference output. \"\n",
    "                \"Acknowledge that variations in expression are acceptable as long as the same core information is captured.\"\n",
    "            )   \n",
    "        )\n",
    "\n",
    "    response = openai_client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are evaluating how well a model's extraction of 10-K document information matches a reference ground truth output. \"\n",
    "                    \"Your task is to determine if the model successfully captured the same essential business information as the reference, \"\n",
    "                    \"understanding that similar concepts may be expressed differently.\\n\\n\"\n",
    "                    \"Context:\\n\"\n",
    "                    \"- The reference output represents the ground truth extraction from a 10-K document\\n\"\n",
    "                    \"- The model's output is being evaluated against this reference for accuracy and completeness\\n\"\n",
    "                    \"- Both extractions contain key business information like products/services, risk factors, and financial metrics\\n\"\n",
    "                    \"- The goal is to verify the model captured similar information as the reference, not identical expression\\n\\n\"\n",
    "                    \"Evaluation Guidelines:\\n\"\n",
    "                    \"- Score 1 (true) if the model's output:\\n\"\n",
    "                    \"  * Captures the same core business information as the reference\\n\"\n",
    "                    \"  * Identifies similar risk factors, even if described differently\\n\"\n",
    "                    \"  * Extracts matching or equivalent financial metrics\\n\"\n",
    "                    \"  * Contains consistent company identifiers\\n\"\n",
    "                    \"  * May include additional valid information beyond the reference\\n\\n\"\n",
    "                    \"- Score 0 (false) only if the model's output:\\n\"\n",
    "                    \"  * Misses or contradicts critical information from the reference\\n\"\n",
    "                    \"  * Shows fundamental misunderstanding of the business details\\n\"\n",
    "                    \"  * Contains irreconcilable differences in key metrics\\n\"\n",
    "                    \"  * Fails to capture the essential information found in the reference\\n\\n\"\n",
    "                    \"Remember: The reference output is our ground truth. Evaluate how well the model's extraction \"\n",
    "                    \"captures the same essential business information, allowing for variations in expression.\\n\\n\"\n",
    "                    \"Outputs to Evaluate:\\n\"\n",
    "                    f\"- **Model Output:** {formatted_text[0]}\\n\"\n",
    "                    f\"- **Reference Ground Truth:** {formatted_text[1]}\\n\"\n",
    "                )\n",
    "            }\n",
    "        ],\n",
    "        response_format=Score\n",
    "    )\n",
    "    response_object = json.loads(response.choices[0].message.content)\n",
    "    return response_object\n",
    "\n",
    "\n",
    "@traceable\n",
    "def evaluate_accuracy(outputs: dict, reference_outputs: dict) -> dict:\n",
    "    actual_output = outputs[\"response\"]\n",
    "    expected_output = reference_outputs['output']\n",
    "    formatted_text = format_objects_for_llm_judge(actual_output, expected_output)\n",
    "    object_response = run_llm_judge(formatted_text)[\"response\"]\n",
    "    return { \"key\": \"accuracy\",\n",
    "            \"score\": object_response[\"accuracy\"],\n",
    "            \"reason\": object_response[\"reason\"] }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithNotFoundError",
     "evalue": "Dataset 10-k extraction not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLangSmithNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m experiment_results \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10-k extraction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mevaluate_accuracy\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10-k-extraction-gpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m experiment_results\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "File \u001b[0;32m~/Documents/GitHub/llm-sdlc/langchain_eval/eval_langchain/lib/python3.9/site-packages/langsmith/client.py:6969\u001b[0m, in \u001b[0;36mClient.evaluate\u001b[0;34m(self, target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, num_repetitions, blocking, experiment, upload_results, **kwargs)\u001b[0m\n\u001b[1;32m   6965\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_runner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate \u001b[38;5;28;01mas\u001b[39;00m evaluate_\n\u001b[1;32m   6967\u001b[0m \u001b[38;5;66;03m# Need to ignore because it fails when there are too many union types +\u001b[39;00m\n\u001b[1;32m   6968\u001b[0m \u001b[38;5;66;03m# overloads.\u001b[39;00m\n\u001b[0;32m-> 6969\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevaluate_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[misc]\u001b[39;49;00m\n\u001b[1;32m   6970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   6971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   6973\u001b[0m \u001b[43m    \u001b[49m\u001b[43msummary_evaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_evaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupload_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupload_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6983\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/llm-sdlc/langchain_eval/eval_langchain/lib/python3.9/site-packages/langsmith/evaluation/_runner.py:427\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, num_repetitions, client, blocking, experiment, upload_results, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m     _warn_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupload_results\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter is in beta.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    426\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning evaluation over target system \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mEVALUATOR_T\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43msummary_evaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_evaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupload_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupload_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/llm-sdlc/langchain_eval/eval_langchain/lib/python3.9/site-packages/langsmith/evaluation/_runner.py:1058\u001b[0m, in \u001b[0;36m_evaluate\u001b[0;34m(target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, num_repetitions, client, blocking, experiment, upload_results)\u001b[0m\n\u001b[1;32m   1051\u001b[0m runs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m _is_callable(target) \u001b[38;5;28;01melse\u001b[39;00m cast(Iterable[schemas\u001b[38;5;241m.\u001b[39mRun], target)\n\u001b[1;32m   1052\u001b[0m experiment_, runs \u001b[38;5;241m=\u001b[39m _resolve_experiment(\n\u001b[1;32m   1053\u001b[0m     experiment,\n\u001b[1;32m   1054\u001b[0m     runs,\n\u001b[1;32m   1055\u001b[0m     client,\n\u001b[1;32m   1056\u001b[0m )\n\u001b[0;32m-> 1058\u001b[0m manager \u001b[38;5;241m=\u001b[39m \u001b[43m_ExperimentManager\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If provided, we don't need to create a new experiment.\u001b[39;49;00m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mruns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mruns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Create or resolve the experiment.\u001b[39;49;00m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_attachments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_include_attachments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_evaluators_include_attachments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupload_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupload_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1072\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m ls_utils\u001b[38;5;241m.\u001b[39mget_cache_dir(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1073\u001b[0m cache_path \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1074\u001b[0m     pathlib\u001b[38;5;241m.\u001b[39mPath(cache_dir) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanager\u001b[38;5;241m.\u001b[39mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cache_dir \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/llm-sdlc/langchain_eval/eval_langchain/lib/python3.9/site-packages/langsmith/evaluation/_runner.py:1441\u001b[0m, in \u001b[0;36m_ExperimentManager.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ExperimentManager:\n\u001b[0;32m-> 1441\u001b[0m     first_example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mislice(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexamples\u001b[49m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   1442\u001b[0m     project \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_project(first_example) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upload_results \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_experiment_start(project, first_example)\n",
      "File \u001b[0;32m~/Documents/GitHub/llm-sdlc/langchain_eval/eval_langchain/lib/python3.9/site-packages/langsmith/evaluation/_runner.py:1403\u001b[0m, in \u001b[0;36m_ExperimentManager.examples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attachment_raw_data_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1398\u001b[0m             \u001b[38;5;28mstr\u001b[39m(e\u001b[38;5;241m.\u001b[39mid) \u001b[38;5;241m+\u001b[39m name: value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreader\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1399\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m examples_copy\n\u001b[1;32m   1400\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m (e\u001b[38;5;241m.\u001b[39mattachments \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1401\u001b[0m         }\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_repetitions \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1403\u001b[0m         examples_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1404\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_examples \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   1405\u001b[0m             [\n\u001b[1;32m   1406\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_example_attachment_readers(example)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_repetitions)\n\u001b[1;32m   1410\u001b[0m         )\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_examples, examples_iter \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mtee(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_examples)\n",
      "File \u001b[0;32m~/Documents/GitHub/llm-sdlc/langchain_eval/eval_langchain/lib/python3.9/site-packages/langsmith/client.py:4495\u001b[0m, in \u001b[0;36mClient.list_examples\u001b[0;34m(self, dataset_id, dataset_name, example_ids, as_of, splits, inline_s3_urls, offset, limit, metadata, filter, include_attachments, **kwargs)\u001b[0m\n\u001b[1;32m   4493\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset_id\n\u001b[1;32m   4494\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4495\u001b[0m     dataset_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mid\n\u001b[1;32m   4496\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset_id\n\u001b[1;32m   4497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/llm-sdlc/langchain_eval/eval_langchain/lib/python3.9/site-packages/langsmith/utils.py:138\u001b[0m, in \u001b[0;36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     invalid_group_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg_groups[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m invalid_groups]\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one argument in each of the following\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m groups must be defined:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m     )\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/llm-sdlc/langchain_eval/eval_langchain/lib/python3.9/site-packages/langsmith/client.py:3296\u001b[0m, in \u001b[0;36mClient.read_dataset\u001b[0;34m(self, dataset_name, dataset_id)\u001b[0m\n\u001b[1;32m   3294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   3295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithNotFoundError(\n\u001b[1;32m   3297\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3298\u001b[0m         )\n\u001b[1;32m   3299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[1;32m   3300\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresult[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   3301\u001b[0m         _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url,\n\u001b[1;32m   3302\u001b[0m         _tenant_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_optional_tenant_id(),\n\u001b[1;32m   3303\u001b[0m     )\n\u001b[1;32m   3304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[1;32m   3305\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresult,\n\u001b[1;32m   3306\u001b[0m     _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url,\n\u001b[1;32m   3307\u001b[0m     _tenant_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_optional_tenant_id(),\n\u001b[1;32m   3308\u001b[0m )\n",
      "\u001b[0;31mLangSmithNotFoundError\u001b[0m: Dataset 10-k extraction not found"
     ]
    }
   ],
   "source": [
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data=\"10-k extraction\",\n",
    "    evaluators=[evaluate_accuracy],\n",
    "    experiment_prefix=\"10-k-extraction-gpt-4o\",\n",
    "    max_concurrency=5,\n",
    "    num_repetitions=3\n",
    ")\n",
    "\n",
    "experiment_results.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
